{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENGR 891: Programming Assignment #3\n",
    "## Part A: \n",
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.0              0.27         0.36            20.7      0.045   \n",
       "1               6.3              0.30         0.34             1.6      0.049   \n",
       "2               8.1              0.28         0.40             6.9      0.050   \n",
       "3               7.2              0.23         0.32             8.5      0.058   \n",
       "4               7.2              0.23         0.32             8.5      0.058   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         8.8        6  \n",
       "1         9.5        6  \n",
       "2        10.1        6  \n",
       "3         9.9        6  \n",
       "4         9.9        6  \n",
       "...       ...      ...  \n",
       "4893     11.2        6  \n",
       "4894      9.6        5  \n",
       "4895      9.4        6  \n",
       "4896     12.8        7  \n",
       "4897     11.8        6  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file as a pandas data frame object\n",
    "df = pd.read_csv('winequality-white.csv', header = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if there is a NAN in data set, clean the data if it is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there is a NAN in data set, clean the data if it is True\n",
    "df.isna().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data frame object for the features and another data frame object for the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.astype of 0       6\n",
      "1       6\n",
      "2       6\n",
      "3       6\n",
      "4       6\n",
      "       ..\n",
      "4893    6\n",
      "4894    5\n",
      "4895    6\n",
      "4896    7\n",
      "4897    6\n",
      "Name: quality, Length: 4898, dtype: int64>\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame object for the features and another data frame object for the target\n",
    "df_features = df.drop(columns=['quality'])  # Data frame containing all features excluding the target & id \n",
    "df_target = df['quality']# Data frame containing the target \n",
    "print(df_target.astype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jing/opt/anaconda3/envs/am_keras_tf/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_counts = df.quality.value_counts()\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.barplot(label_counts.index, label_counts.values, alpha=0.9)\n",
    "\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.xlabel('quality', fontsize=12)\n",
    "plt.ylabel('Counts', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection: using pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Correlated Features:\n",
      "\n",
      "quality                 1.000000\n",
      "alcohol                 0.435575\n",
      "density                 0.307123\n",
      "chlorides               0.209934\n",
      "volatile acidity        0.194723\n",
      "total sulfur dioxide    0.174737\n",
      "fixed acidity           0.113663\n",
      "pH                      0.099427\n",
      "residual sugar          0.097577\n",
      "sulphates               0.053678\n",
      "citric acid             0.009209\n",
      "free sulfur dioxide     0.008158\n",
      "Name: quality, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Variable Correlations with the target \n",
    "most_correlated = df.corr().abs()['quality'].sort_values(ascending=False) # the absolute value\n",
    "# Maintain the top 11 most correlation features with target\n",
    "most_correlated = most_correlated[:12]\n",
    "print(\"Most Correlated Features:\\n\")\n",
    "print(most_correlated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1 has the most correlated 10 features test accuracy (best of trials)\n",
    "df_case1 = df[['alcohol','density','chlorides','volatile acidity','total sulfur dioxide','fixed acidity','pH','residual sugar','sulphates','citric acid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the above two data frame objects into two NumPy arrays¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the above two data frame objects into two NumPy arrays\n",
    "X = np.asarray(df_case1) # Data Matrix containing optimal features excluding the target\n",
    "y = np.asarray(df_target) # Data Matrix of the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the target array type into “int”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4898, 10)\n",
      "(4898,)\n",
      "\n",
      "X data type:  float64\n",
      "y data type:  int64\n"
     ]
    }
   ],
   "source": [
    "# Convert the target array type into “int”.\n",
    "y = y.astype(int)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(\"\\nX data type: \", X.dtype)\n",
    "print(\"y data type: \", y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition the dataset into training & test subsets: 80% training & 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the dataset into training & test subsets: 80% training & 20% test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarized the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standarized the data \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: K-Nearest Neighbors (K-NN)\n",
    "### Model selection: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 490 candidates, totalling 2450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 288 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 788 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1488 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2388 tasks      | elapsed:  1.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.671771\n",
      "Optimal Hyperparameter Values:  {'n_neighbors': 23, 'p': 1, 'weights': 'distance'}\n",
      "\n",
      "\n",
      "CPU times: user 3.54 s, sys: 302 ms, total: 3.84 s\n",
      "Wall time: 1min 37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 2450 out of 2450 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# The param_grid tells Scikit-Learn to evaluate all combinations of the hyperparameter values\n",
    "param_grid = {'n_neighbors': np.arange(1,50), 'p': [1, 2, 3, 100, 200], 'weights': [\"uniform\", \"distance\"]}\n",
    "\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "knn_cv = GridSearchCV(knn_clf, param_grid, scoring='f1_micro', cv=5, verbose=1, n_jobs=-1)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "params_optimal_knn = knn_cv.best_params_\n",
    "\n",
    "print(\"Best Score: %f\" % knn_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_knn)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Using the optimal hyperparameter values, create the best model. Then, fit the model.\n",
    "\n",
    "knn = KNeighborsClassifier(**params_optimal_knn)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Train accuracy of the model\n",
    "y_train_predicted = knn.predict(X_train)\n",
    "\n",
    "train_accuracy_knn = np.mean(y_train_predicted == y_train)\n",
    "print(\"\\nTraining Accuracy: \", train_accuracy_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:  0.6346938775510204\n",
      "\n",
      "No. of correct predictions (Test): 622/980\n",
      "CPU times: user 187 ms, sys: 2.36 ms, total: 189 ms\n",
      "Wall time: 188 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Test accuracy of the model\n",
    "test_accuracy_knn = knn.score(X_test, y_test)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy_knn)\n",
    "\n",
    "\n",
    "# No. of Correct Predictions\n",
    "y_test_predicted = knn.predict(X_test)\n",
    "print(\"\\nNo. of correct predictions (Test): %d/%d\" % (np.sum(y_test_predicted == y_test), len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of correct predictions (Test): 622/980\n",
      "\n",
      "Confusion Matrix (Test Data):\n",
      " [[  0   0   4   5   0   0]\n",
      " [  0   4  27  18   2   0]\n",
      " [  0   0 193  94   8   0]\n",
      " [  0   0  54 319  34   2]\n",
      " [  0   0   4  80  99   0]\n",
      " [  0   0   1  16   9   7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         9\n",
      "           4       1.00      0.08      0.15        51\n",
      "           5       0.68      0.65      0.67       295\n",
      "           6       0.60      0.78      0.68       409\n",
      "           7       0.65      0.54      0.59       183\n",
      "           8       0.78      0.21      0.33        33\n",
      "\n",
      "    accuracy                           0.63       980\n",
      "   macro avg       0.62      0.38      0.40       980\n",
      "weighted avg       0.66      0.63      0.61       980\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 103 ms, sys: 1.89 ms, total: 105 ms\n",
      "Wall time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# No. of Correct Predictions\n",
    "y_test_predicted = knn.predict(X_test)\n",
    "print(\"\\nNo. of correct predictions (Test): %d/%d\" % (np.sum(y_test_predicted == y_test), len(y_test)))\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (Test Data):\\n\", confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "# Classification report \n",
    "\n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection for OvA: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 352 tasks      | elapsed:    4.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (F1 score): 0.540321\n",
      "Optimal Hyperparameter Values:  {'C': 0.1, 'max_iter': 500, 'multi_class': 'ovr', 'solver': 'liblinear', 'tol': 0.001}\n",
      "\n",
      "\n",
      "CPU times: user 806 ms, sys: 53.6 ms, total: 860 ms\n",
      "Wall time: 6.38 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:    6.3s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'solver': ['liblinear', 'newton-cg', 'lbfgs'], \n",
    "              'multi_class' : ['ovr'],\n",
    "              'tol': [1e-3, 1e-4], 'max_iter':[500, 1000],'C': [0.1, 0.5, 1, 10, 15, 50, 100, 500]}\n",
    "\n",
    "lg_reg = LogisticRegression()\n",
    "\n",
    "lg_reg_cv = GridSearchCV(lg_reg, param_grid, scoring='f1_micro', cv=5, verbose=1, n_jobs=-1)\n",
    "lg_reg_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal = lg_reg_cv.best_params_\n",
    "\n",
    "print(\"Best Score (F1 score): %f\" % lg_reg_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Optimal Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=500, multi_class='ovr', solver='liblinear',\n",
       "                   tol=0.001)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_reg_ova = LogisticRegression(**params_optimal)\n",
    "\n",
    "lg_reg_ova.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Optimal Classifier on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Iterations: [8]\n",
      "Training Accuracy:  0.5436447166921899\n",
      "\n",
      "Test Accuracy:  0.4959183673469388\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[  0   0   6   3   0   0]\n",
      " [  0   1  29  20   1   0]\n",
      " [  0   0 137 158   0   0]\n",
      " [  0   0  70 331   8   0]\n",
      " [  0   0   8 158  17   0]\n",
      " [  0   0   0  29   4   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         9\n",
      "           4       1.00      0.02      0.04        51\n",
      "           5       0.55      0.46      0.50       295\n",
      "           6       0.47      0.81      0.60       409\n",
      "           7       0.57      0.09      0.16       183\n",
      "           8       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.50       980\n",
      "   macro avg       0.43      0.23      0.22       980\n",
      "weighted avg       0.52      0.50      0.43       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of Iterations:\",lg_reg_ova.n_iter_ )\n",
    "\n",
    "# Train accuracy of the model\n",
    "y_train_predicted = lg_reg_ova.predict(X_train)\n",
    "\n",
    "print(\"Training Accuracy: \", lg_reg_ova.score(X_train, y_train))\n",
    "\n",
    "y_test_predicted = lg_reg_ova.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Polynomial Logistic Regression\n",
    "### Model Selection: Hyperparameter Tuning via Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   32.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 11.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score f1_micro: 0.557165\n",
      "Optimal Hyperparameter Values:  {'log__C': 1, 'log__max_iter': 500, 'log__multi_class': 'ovr', 'log__solver': 'liblinear', 'log__tol': 0.0001, 'poly__degree': 2}\n",
      "\n",
      "\n",
      "CPU times: user 2.11 s, sys: 107 ms, total: 2.22 s\n",
      "Wall time: 11min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a Pipeline object\n",
    "log_pipeline = Pipeline([\n",
    "        # Bias was included for BD logistic regression\n",
    "        ('poly', PolynomialFeatures(include_bias=False)), \n",
    "        ('log', LogisticRegression()),\n",
    "    ])\n",
    "\n",
    "# Create a dictionary object with hyperparameters as keys and lists of corresponding values\n",
    "param_grid = {'poly__degree': [2, 3],\n",
    "              'log__solver': ['liblinear', 'newton-cg', 'lbfgs'], \n",
    "              'log__multi_class' : ['ovr'],\n",
    "              'log__tol': [1e-3, 1e-4], \n",
    "              'log__max_iter':[500,1000],\n",
    "              'log__C': [1, 10, 100]}\n",
    "\n",
    "# Create a GridSearchCV object and perform hyperparameter tuning\n",
    "log = GridSearchCV(log_pipeline, param_grid, scoring='f1_micro', cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "# The model is trained with optimal hyperparameters, thus its the optimal model\n",
    "log.fit(X_train, y_train)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "params_optimal_log = log.best_params_\n",
    "\n",
    "print(\"Best Score f1_micro: %f\" % log.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_log)\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation: evaluation based on training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.5436447166921899\n",
      "\n",
      "Test Accuracy:  0.5255102040816326\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[  0   0   4   4   1   0]\n",
      " [  0   6  27  17   1   0]\n",
      " [  0   0 153 140   2   0]\n",
      " [  0   1  77 300  29   2]\n",
      " [  0   0   8 121  53   1]\n",
      " [  0   0   0  23   7   3]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         9\n",
      "           4       0.86      0.12      0.21        51\n",
      "           5       0.57      0.52      0.54       295\n",
      "           6       0.50      0.73      0.59       409\n",
      "           7       0.57      0.29      0.38       183\n",
      "           8       0.50      0.09      0.15        33\n",
      "\n",
      "    accuracy                           0.53       980\n",
      "   macro avg       0.50      0.29      0.31       980\n",
      "weighted avg       0.55      0.53      0.50       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted = log.predict(X_test)\n",
    "#print(y_test_predict)\n",
    "\n",
    "# Train accuracy of the model\n",
    "y_train_predicted = log.predict(X_train)\n",
    "print(\"Training Accuracy: \", lg_reg_ova.score(X_train, y_train))\n",
    "\n",
    "\n",
    "accuracy_score_test = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Kernelized SVM with Polynomial Kernel\n",
    "### Model Selection: Hyperparameter Tuning via Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (f1_micro): 0.598011\n",
      "Optimal Hyperparameter Values:  {'poly__degree': 2, 'svc__C': 1000, 'svc__gamma': 0.01}\n",
      "\n",
      "\n",
      "CPU times: user 4.67 s, sys: 129 ms, total: 4.8 s\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a Pipeline object\n",
    "svc_pipeline = Pipeline([\n",
    "        ('poly', PolynomialFeatures(include_bias= False)), \n",
    "        ('svc', SVC(kernel ='poly')),\n",
    "    ])\n",
    "\n",
    "# Create a dictionary object with hyperparameters as keys and lists of corresponding values\n",
    "param_grid = {'poly__degree': [2, 3, 4],\n",
    "              'svc__gamma': [0.001, 0.01, 0.1],\n",
    "              'svc__C': [10, 1000]}\n",
    "\n",
    "# Create a GridSearchCV object and perform hyperparameter tuning\n",
    "svc_clf= GridSearchCV(svc_pipeline, param_grid, scoring='f1_micro', cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "# The model is trained with optimal hyperparameters, thus its the optimal model\n",
    "svc_clf.fit(X_train, y_train)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "params_optimal_svc_clf = svc_clf.best_params_\n",
    "\n",
    "print(\"Best Score (f1_micro): %f\" % svc_clf.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_svc_clf)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9310872894333844\n",
      "Test Accuracy:  0.5591836734693878\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[  1   0   3   4   1   0   0]\n",
      " [  1  12  18  17   1   0   2]\n",
      " [  2  10 152 124   6   0   1]\n",
      " [  4   8  58 294  35   9   1]\n",
      " [  1   3  15  74  81   9   0]\n",
      " [  0   2   2  15   6   8   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.11      0.11      0.11         9\n",
      "           4       0.34      0.24      0.28        51\n",
      "           5       0.61      0.52      0.56       295\n",
      "           6       0.56      0.72      0.63       409\n",
      "           7       0.62      0.44      0.52       183\n",
      "           8       0.31      0.24      0.27        33\n",
      "           9       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.56       980\n",
      "   macro avg       0.36      0.32      0.34       980\n",
      "weighted avg       0.56      0.56      0.55       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_predicted = svc_clf.predict(X_train)\n",
    "\n",
    "print(\"Training Accuracy: \", svc_clf.score(X_train, y_train))\n",
    "y_test_predicted = svc_clf.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy: \", svc_clf.score(X_test, y_test))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Kernelized SVM with Gaussian Radial Basis Function (RBF)\n",
    "### Model Selection: Hyperparameter Tuning via Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (f1_micro): 0.638588\n",
      "Optimal Hyperparameter Values:  {'C': 10, 'gamma': 1}\n",
      "\n",
      "\n",
      "CPU times: user 1.43 s, sys: 99.1 ms, total: 1.53 s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {'gamma': [0.000001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
    "              'C': [10, 100, 1000, 10000]}\n",
    "clf = SVC()\n",
    "\n",
    "clf_cv = GridSearchCV(clf, param_grid, scoring='f1_micro', cv=5, n_jobs=-1)\n",
    "clf_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal = clf_cv.best_params_\n",
    "\n",
    "print(\"Best Score (f1_micro): %f\" % clf_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9966819806023481\n",
      "Test Accuracy:  0.6316326530612245\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[  0   0   0   9   0   0]\n",
      " [  0   5  16  27   3   0]\n",
      " [  0   3 186  99   7   0]\n",
      " [  0   0  51 315  41   2]\n",
      " [  0   0   8  68 104   3]\n",
      " [  0   0   2  16   6   9]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         9\n",
      "           4       0.62      0.10      0.17        51\n",
      "           5       0.71      0.63      0.67       295\n",
      "           6       0.59      0.77      0.67       409\n",
      "           7       0.65      0.57      0.60       183\n",
      "           8       0.64      0.27      0.38        33\n",
      "\n",
      "    accuracy                           0.63       980\n",
      "   macro avg       0.54      0.39      0.42       980\n",
      "weighted avg       0.63      0.63      0.61       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_rbf_clf = SVC(kernel=\"rbf\", **params_optimal)\n",
    "svm_rbf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_predicted = svm_rbf_clf.predict(X_train)\n",
    "\n",
    "print(\"Training Accuracy: \", svm_rbf_clf.score(X_train, y_train))\n",
    "y_test_predicted = svm_rbf_clf.predict(X_test)\n",
    "print(\"Test Accuracy: \", svm_rbf_clf.score(X_test, y_test))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-1) Which model did you find most effective (optimal test performance)? Explain why this model performed better than other models in your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though KNN and RFB model have provided very close results, we concluded that RBF kernel has the best performance among the models due to lesser time for hyperparameter tuning. This data set generally is not linearly separated because the logistic regression is not performing well (underfit as train & test accuracy is bad). However, overall, these models do not perform well in classifying the data. We believed that the data is too complex for the limited hyperparameter tuning ranges of models to provide higher performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-2) Which model did you find most efficient (less time for hyperparameter tuning)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-3) Which model achieved the best precision and recall score for class 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernelized SVM with Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-4) How could you improve the precision and recall for class 3? Justify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do that is to use more training data for class 3. However, sometimes the data is limited. Machine learning with SMOTE function has been proved is another way to deal with unbalanced data (i.e., class 3). SMOTE is a type of data augmentation that synthesizes new samples from the existing ones. SMOTE actually can create new samples. Therefore, we can use it to oversample the minority class 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
